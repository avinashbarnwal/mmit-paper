Max margin interval trees

** TODOs

- For what kinds of data does MMIT with the square loss learn a more
  accurate model than Interval-CART?

** 16 June 2017
- Emailed Torsten Hothorn, author of trafotree, with this bug
  description [[file:figure-trafotree-bug.R]]. Test error bigger than
  constant model for one data set. 
- He emailed back, with this code to solve the problem
  [[file:figure-trafotree-bug-response.R]]. Apparently when we set up the
  basis function for the output, we need to force the variance to be
  positive (positive slope, achieved via ui/ci arguments to as.basis),
  and we need to partition only on intercept (not slope/variance,
  achieved by parm=1 in trafotree). after redoing the test error
  figure, this does indeed fix the issue (trafotree is now learning as
  well as IntervalRegressionCV).
** 10 May 2017
Lots more algos and data sets, prediction accuracy figure
http://bl.ocks.org/tdhock/raw/75751a85d2766cd43be4c36ee3fa58a1/
** 31 Mar 2017

Created a new filter when making data sets (observations must be
greater than 13), which removed H3K36me3_TDH_other_joint (it had one
fold with no negative labels, so we can't compute AUC). So now there
are 24 data sets in
http://cbio.ensmp.fr/~thocking/data/penalty-learning-interval-regression-problems.tgz

[[file:figure-evaluate-predictions.R]] creates figures that compare the
prediction accuracy. Right now I have just computed
IntervalRegressionCV (linear model trained by minimizing squared hinge
loss + L1 penalty) and constant (baseline model that just learns the
constant penalty with minimum incorrect target intervals). We can see
that IntervalRegressionCV does better in the tall data setting, and
does about the same in the fat data setting.

** 29 Mar 2017 

[[file:penaltyLearning.predictions.R]] creates predictions files for
IntervalRegresionCV (linear model with squared hinge loss + L1
regularization).

To make it easy to compare models which we fit in either R or Python, I would suggest that we save model predictions in the following format. Create a separate directory called "predictions" inside of which is one sub-directory for each model. Each model sub-directory would have another sub-directory for each data set, in which there is a predictions.csv file (n x 1 -- predicted values for each observation in 5-fold CV). For example

project/data/lymphoma.mkatayama/features.csv
project/predictions/mmit.linear.hinge/lymphoma.mkatayama/predictions.csv
project/predictions/mmit.squared.hinge/lymphoma.mkatayama/predictions.csv
etc

26 penalty learning data sets created via [[file:data.sets.R]] (but one is
less than 10 observations so we ignore it, leaving a total of 25 data
sets). It creates a data directory with a subdirectory for each data
set. Inside each of those are three files
1. targets.csv is the n x 2 matrix of target intervals (outputs).
2. features.csv is the n x p matrix of features (inputs). p is
   different for each data set.
3. folds.csv is a n x 1 vector of fold IDs -- for comparing model
   predictions using 5-fold cross-validation.

- R pkg neuroblastoma + labels.
- http://members.cbio.ensmp.fr/~thocking/neuroblastoma/signal.list.annotation.sets.RData
  this data contains many different types of microarrays -- maybe
  create a data set that groups them all together?
- thocking@guillimin:PeakSegFPOP/ChIPseq.wholeGenome.rds contains
  features + targets for genome wide ChIP-seq segmentation models
  (PeakSegFPOP and PeakSegJoint).
- TODO copy 7 benchmark data sets from work computer. TO benchmark web
  page. Scripts to compute [[https://github.com/tdhock/PeakSegFPOP-paper/blob/master/PDPA.targets.R][targets]] and [[https://github.com/tdhock/PeakSegFPOP-paper/blob/master/problem.features.R][features]].

[[file:figure-data-set-sizes.R]] shows a summary of the dimensions of the
25 data sets, each of which should be treated as a separate learning
problem.
- the number of features varies from 26 to 259.
- the number of observations varies from 13 to 3418.
- some data sets are "fat" (n < p) and others are tall (p < n)
- some data sets have more upper limits, others have more lower limits.
- the penalty functions are for four types of segmentation models.

[[file:figure-data-set-sizes.png]]

** 22 March 2017

[[http://bl.ocks.org/tdhock/raw/105352ef496c22a80aea7c326b64c0a3/][Interactive figure]]: select threshold on total cost curves, see updated
prediction, margin and slack.

** 16 March 2017

[[file:figure-penaltyLearning.R]] visualizes cost as a function of feature
value.
